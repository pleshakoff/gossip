# GOSSIP
## Синхронизациия данных чата между узлами по протоколу GOSSIP  

1. [Описание системы](#desc)
2. [Get started](#get-started)
3. [Примеры](#examples)


<a name="desc"></a>
## Описание системы. 

![alt text](https://github.com/pleshakoff/gossip/blob/master/GOSSIP.png?raw=true"")

Распределенная AP система для хранения сообщений, отправленных в чат.
Для репликации данных используется протокол GOSSIP.
Реализация написана на Java+Spring Boot.
 
Система состоит из двух модулей, для двух типов нод: клиент и сервер. 
Можно развернуть неограниченное количество инстансов как сервера так и клиента.
В текущей конфигурации настроены 5 нод и 1 клиент. 
Топология сети показана на рисунке выше. Топология настраивается в конфигурационном файле 
(подробнее [конфигурирование](#config))

  

### Сервер

https://github.com/pleshakoff/gossip/tree/master/server

Доступны пять нод. Идентификаторы нод: 1,2,3,4,5

С API можно работать через swagger(подробенее в разделе [API](#api))

#### Описание 

Серверная нода умет добавлять данные в чат и показывать все записи чата. 

Каждая серверная нода имеет доступ к БД, в которой хранятся непосредственно данные. 

БД у каждой ноды своя отдельная. 

В текущей реализации используется embedded in-memory решения для БД.
(конкурентный List). Так же для быстрой проверки есть ли запись уже в чате, 
в качестве инедекса БД используется HashSet (O(1) для  операции contains)    
В случае необходимости можно просто имплементировать соответствующий интерфейс 
для поддержки иных типов хранилищ.

На серверной ноде запущен таймер для запуска gossip взаимодействия. 
В текущей реализации выбрана стратегия pull.  

Каждая нода по таймеру опрашивает своих соседей на предмет наличия новых записей. 
Для хранения состояния данных каждой ноды используется концепция "логические часы". 

Текущая версия данных и версии соседних нод (с точки зрения текущей ноды) хранятся 
в специальном векторе, любое изменение данных ноды приводит к увеличению версии в векторе.

Если данные ноды менялись по результатам pull запроса к соседней ноде, то в векторе повышается версия
данных соседней ноды и текущая версия ноды + 1.   
       
При запросе данных нода отправляет ноде поставщику свой вариант версии данных ноды поставщика, 
и если у ноды поставщика версия уже выше чем присланная, то она в ответ присылает новые данные.
Важно что версия должна быть не просто выше. Текущая версия ноды поставщика 
также должна быть выше версии ноды получателя в векторе ноды поставшика более чем на 1
(или же версия ноды получателя в векторе поставщика = 0). 
Это означает что эти новые изменения были получены не с ноды получателя и их еще нет у получателя 
и их можно отдать получателю. 

Рассмотрим пример с двумя узлами 

Есть две ноды
 
* 1[0,0]
* 2[0,0]  

Добавляем в первую 1 запись 

* 1 [1,0]
* 2 [0,0]  

Вторая нода запрашивает данные у первой, отправляя номер версии 0 и в ответ получает новую запись 
(версия ноды №2(получателя) в векторе ноды №1(поставщика) равна 0, поэтому данные можно отправить). 
При вставке каждой новой записи нода №2 увеличивает номер своей версии в векторе и номер версии узла №1.
И еще сверху добавляет единицу, чтобы показать что данные вставлялись в результате обмена данных   
         
* 1 [1,0]
* 2 [1,2]  

Первая нода запрашивает данные у второй посылая версию 0. И в ответ получает 0 записей. 
Так как версия версия ноды поставщика в векторе поставщика не больше версии ноды получателя+1    
* 1 [1,0]
* 2 [1,2]  

Так как данные на ноду могут прийти разными путями, то все равно, во избежании зацикливания необходимо
обеспечить проверку.

У каждой записи в чате есть массив "visited" со списком узлов через которые прошла запись. 
Если запись уже была на этом узле она не вставляется. 

Также у каждой записи есть поле "version" в котором содержится локальная версия внутри ноды, соответсвующая 
версии на логических часах в момент вставки.
Версия нужна для вытягивания нужных данных по запросу соседними нодами.     
 
Для того чтобы можно было эмулировать отключения нод без опускания контейнеров, есть возможность через API
останавливать ноды. После остановки нода молчит, она недоступна для других нод и для записи клиентом.  
Но есть возможность получить содержимое чата, что удобно для исследований поведения кластера 
в нешататной ситуации. 

      
<a name="api"></a>            
#### API 

Нода #1:  http://localhost:8081/api/v1/swagger-ui.html

Нода #2:  http://localhost:8082/api/v1/swagger-ui.html

Нода #3:  http://localhost:8083/api/v1/swagger-ui.html

Нода #4:  http://localhost:8084/api/v1/swagger-ui.html

Нода #5:  http://localhost:8085/api/v1/swagger-ui.html
   
В API доступны следующее группы методов 

* Context. Получение метаданных ноды. Остановка/запуск ноды 
* Chat. Добавление сообщения в чат. Просмотр чата.
* Gossip. Эндпоинт для pull запросов

#### Реализация 

Пакеты:

* [node](https://github.com/pleshakoff/gossip/tree/master/server/src/main/java/com/gossip/server/node). 
Метаданные узла. Данные нодов соседей. Логическое время    
* [exchange](https://github.com/pleshakoff/gossip/tree/master/server/src/main/java/com/gossip/server/exchange). 
Сервис для отправки и обработки gossip pull реквеста.   
* [storage](https://github.com/pleshakoff/gossip/tree/master/server/src/main/java/com/gossip/server/storage). 
Интерфейс для доступа к БД. Его in memory реализация. Сервис для операций с БД. 
* [context](https://github.com/pleshakoff/gossip/tree/master/server/src/main/java/com/gossip/server/context). 
Декоратор для удобного доступа к метаданным узла.  
* [timer](https://github.com/pleshakoff/gossip/tree/master/server/src/main/java/com/gossip/server/timer). 
Таймер для gossip обмена  

  
### Клиент

https://github.com/pleshakoff/gossip/tree/master/client

В текущей конфигурации запускается в единственном экземпляре. 
С API можно работать через swagger(подробенее в разделе [API](#apiclient))

#### Описание 

Отправляет запросы серверу. 
Может собрать метаданные со всего кластера и показать доступные ноды и их состояния. 

Чтение и запись можно осуществлять на любую ноду.  
В текущей реализации клиент не умеет сам решать какую ноду вызвать, 
это надо указать в параметре запроса, так сделано специально 
чтобы удобно было исследовать поведение разных нод.

При записи, добавляется запись в БД выбранной ноды. 
После этого данные постепенно синхронизируются на всех остальных узлах

      
<a name="apiclient"></a>            
#### API 

Клиент:  http://localhost:8080/api/v1/swagger-ui.html
   
В API доступны следующее группы методов 

* Context. Получение метаданных с всего кластера. Остановка/запуск нод. 
* Chat. Добавление сообщения в чат. Просмотр чата   

#### Реализация 


Пакеты

* [exchange](https://github.com/pleshakoff/gossip/tree/master/client/src/main/java/com/gossip/client/exchange). 
Сервис для получения метаданных серверных нод     

Все остальное это просто редиректы к ендпоинтам серверных нод для чтения и записи данных.  

    


<a name="get-started"></a>
## Get started 

В корне репозитория лежит [docker-compose.yml](https://github.com/pleshakoff/gossip/blob/master/docker-compose.yml)
его надо запустить, поднимается пять серверных нод и клиент.

` docker-compose up`


<a name="config"></a>
##### Конфигурирование 

В корне репозитория лежат конфигурационные файлы для серверной ноды 
[config_server.yml](https://github.com/pleshakoff/gossip/blob/master/config_server.yml)
и для клиентской ноды 
[config_client.yml](https://github.com/pleshakoff/gossip/blob/master/config_client.yml).

При запуске docker-compose собираются локальные контейнеры(на базе контейнеров приложения из docker hub) 
и файл конфигурации копируется в этот контейнер(см. 
[Dockerfile_server](https://github.com/pleshakoff/gossip/blob/master/Dockerfile_server),
[Dockerfile_client](https://github.com/pleshakoff/gossip/blob/master/Dockerfile_client)). 
В текущей реализации для всех нод используется один и тот же файл, 
но внутри он разбит на секции, соответствующие разным профилям. Одна нода - один профиль. 
В docker-compose.yml в параметрах запуска для каждый ноды указывается свой профиль.

Если в конфигурационный файл вносились измнения то необходимо пересобрать контейнеры  
Например при запуске, указав флаг `docker-compose up --build` 
 
Если необходимо, можно увеличить количество нод или изменить их взаимосвязи. 
Также можно для каждой ноды сделать свой отдельный конфигурациронный файл, и свой docker файл. 

##### Swagger 

Нода #1:  http://localhost:8081/api/v1/swagger-ui.html

Нода #2:  http://localhost:8082/api/v1/swagger-ui.html

Нода #3:  http://localhost:8083/api/v1/swagger-ui.html

Нода #4:  http://localhost:8084/api/v1/swagger-ui.html

Нода #5:  http://localhost:8085/api/v1/swagger-ui.html

Клиент:  http://localhost:8080/api/v1/swagger-ui.html

GET запросы можно запускать прямо в браузере. 
Например получить состояние нод можно по ссылке: http://localhost:8080/api/v1/context

##### Pool timeout 

Для всех нод 2 секунды. 
Таймауты можно перенастроить в конфигурационнных файлах

##### Логи 

`docker-compose logs -f gossip-server-1 `

`docker-compose logs -f gossip-server-2`
 
`docker-compose logs -f gossip-server-3` 

`docker-compose logs -f gossip-server-4` 

`docker-compose logs -f gossip-server-5` 


<a name="examples"></a>
## Примеры

Ниже рассмотрен ряд примеров работы с кластером  

Все примеры нужно выполнять через swagger клиентской ноды.
Все то же самое можно сделать, обращаясь непосредстваенно к серверным  нодам, но через клиента удобнее.   

При отключении узла через API, добавление сообщений недоступно. Но операция просмотра чата и получение метаданных не блокируются.

Для добавления сообщений в ноду №1 необходимо использовать 

**POST** http://localhost:8080/api/v1/chat?peerId=1  

Для получения всего чата ноды №1 

http://localhost:8080/api/v1/chat?peerId=1

Как проходит репликация можно проверить, добавляя сообщения на разные ноды и получая состав чата с разных нод. 
Чат должен быть везде идентичен.

Можно останавливать 
   
**POST** http://localhost:8080/api/v1/context/stop?peerId=1
   
и запускать ноды 

**POST** http://localhost:8080/api/v1/context/start?peerId=1
 
После запуска, нода должна подтянуть данные чата у соседей. 

Для проверки также можно использовать метод, возвращающий данные всех нод

http://localhost:8080/api/v1/context 

Для каждой ноды он возвращает статус, список соседей и логические часы. 
Также есть удобное поле storageSize, показывающее размер чата у каждой ноды.  





  
 
 
